{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_resnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4lKXnO9c4/21zhMgmP9YF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gotjd709/cv_model_torch/blob/master/classification_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ResNet\n"
      ],
      "metadata": {
        "id": "Qw4hfP22VcCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) No Residual Learning (Plain Network)\n",
        "\n",
        "### 1-1) PlainBlock \n",
        "=> 18 layers, 34 layers\n",
        "- Whenever the layer name is changed, the number of input planes of the first block chages. \n",
        "- The exception is that the first layer name is changed.\n",
        "\n",
        "### 1-2) BottlePlain \n",
        "=> 52 layers, 101 layers, 152 layers\n",
        "- It is the same as in PlainBlock, but there is a 1x1 convolution, so more convolution layers should be considered."
      ],
      "metadata": {
        "id": "VjaAT0BvmzrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def conv1x1(ic, oc, s):\n",
        "    return nn.Conv2d(ic, oc, 1, s)\n",
        "\n",
        "def conv3x3(ic, oc, s, p):\n",
        "    return nn.Conv2d(ic, oc, 3, s, p) \n",
        "\n",
        "class PlainBlock(nn.Module):\n",
        "    def __init__(self, ln, bn, fn, s, p):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(fn*(2**(ln-1)), fn*(2**(ln)), s*2, p) if ln != 0 else conv3x3(fn*(2**(ln)), fn*(2**(ln)), s*2, p)\n",
        "        self.conv2 = conv3x3(fn*(2**(ln)), fn*(2**(ln)), s, p)\n",
        "        self.batch = nn.BatchNorm2d(fn*(2**ln))\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.ln = ln\n",
        "        self.bn = bn\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (self.ln != 0) and (self.bn == 1):\n",
        "            conv1  = self.conv1(x)\n",
        "        else:\n",
        "            conv1  = self.conv2(x)\n",
        "        batch1 = self.batch(conv1)\n",
        "        relu1  = self.relu(batch1)\n",
        "        conv2  = self.conv2(relu1)\n",
        "        batch2 = self.batch(conv2)\n",
        "        relu2  = self.relu(batch2)\n",
        "        return relu2\n",
        "\n",
        "class BottlePlain(nn.Module):\n",
        "    def __init__(self, ln, bn, fn, s, p):\n",
        "        super().__init__()\n",
        "        self.conv11= conv1x1(fn*(2**(ln)), fn*(2**(ln)), s)\n",
        "        self.conv12= conv1x1(fn*(2**(ln)*2), fn*(2**(ln)), s) \n",
        "        self.conv13= conv1x1(fn*(2**(ln)*4), fn*(2**(ln)), s)\n",
        "        self.conv2 = conv3x3(fn*(2**(ln)), fn*(2**(ln)), s*2, p) if (ln != 0) and (bn == 1) else conv3x3(fn*(2**(ln)), fn*(2**(ln)), s, p)\n",
        "        self.conv3 = conv1x1(fn*(2**(ln)), fn*(2**(ln))*4, s)\n",
        "        self.batch = nn.BatchNorm2d(fn*(2**ln))\n",
        "        self.batch2= nn.BatchNorm2d(fn*(2**ln)*2)\n",
        "        self.batch4= nn.BatchNorm2d(fn*(2**ln)*4)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.ln = ln\n",
        "        self.bn = bn\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (self.ln == 0) and (self.bn == 1):\n",
        "            conv1 = self.conv11(x)\n",
        "        elif (self.ln != 0) and (self.bn == 1):\n",
        "            conv1 = self.conv12(x)\n",
        "        else:\n",
        "            conv1 = self.conv13(x)\n",
        "        batch1 = self.batch(conv1)\n",
        "        relu1  = self.relu(batch1)\n",
        "        conv2  = self.conv2(relu1)\n",
        "        batch2 = self.batch(conv2)\n",
        "        relu2  = self.relu(batch2)\n",
        "        conv3  = self.conv3(relu2)\n",
        "        batch4 = self.batch4(conv3)\n",
        "        relu3  = self.relu(batch4)\n",
        "        return relu3\n",
        "\n",
        "\n",
        "class PlainNet(nn.Module):\n",
        "    '''\n",
        "    PlainNet is ResNet without residual learning.\n",
        "\n",
        "    Args:\n",
        "        btype: Type of the block that will be used as an residual learning.\n",
        "            Available option: **PlainBlock** (for PlainNet18, PlainNet34) and **BottlePlain(Plain50, PlainNet101, PlainNet152)**\n",
        "        block: List of the block number. (e.g PlainNet34 -> [3,4,6,3])\n",
        "        fn: Number of features. default is 64.\n",
        "        s: stride.\n",
        "        p: padding.\n",
        "        classes: number of classes.\n",
        "    '''\n",
        "    def __init__(self, btype, block, fn, s, p, classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,fn,7,2,3)\n",
        "        self.batch = nn.BatchNorm2d(fn)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxp1 = nn.MaxPool2d(2,2,0)\n",
        "        self.block1 = self._make_layer(btype, 0, block[0], fn, s, p)\n",
        "        self.block2 = self._make_layer(btype, 1, block[1], fn, s, p)\n",
        "        self.block3 = self._make_layer(btype, 2, block[2], fn, s, p)\n",
        "        self.block4 = self._make_layer(btype, 3, block[3], fn, s, p)\n",
        "        self.aapool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.flaten = nn.Flatten()\n",
        "        self.adjust = 1 if btype == 'PlainBlock' else 4\n",
        "        self.linear = nn.Linear(in_features=(fn*2**(len(block)-1)*self.adjust),out_features=classes)\n",
        "\n",
        "    def _make_layer(self, btype, ln, block, fn, s, p):\n",
        "        layers = []\n",
        "        for bn in range(1,block+1):\n",
        "            layers.append(PlainBlock(ln, bn, fn, s, p)) if btype == 'PlainBlock' else layers.append(BottlePlain(ln, bn, fn, s, p))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxp1(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.aapool(x)\n",
        "        x = self.flaten(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "def plain18(btype='PlainBlock', block=[2,2,2,2], fn=64, s=1, p=1, classes=1000):\n",
        "    return PlainNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def plain34(btype='PlainBlock', block=[3,4,6,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return PlainNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def plain50(btype='BottlePlain', block=[3,4,6,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return PlainNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def plain101(btype='BottlePlain', block=[3,4,23,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return PlainNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def plain152(btype='BottlePlain', block=[3,8,36,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return PlainNet(btype, block, fn, s, p, classes)"
      ],
      "metadata": {
        "id": "hYFWkDFAKojI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) ResNet\n",
        "\n",
        "### 2-1) BasicBlock \n",
        "=> 18 layers, 34 layers\n",
        "- Whenever the layer name is changed, the number of input planes of the first block chages. \n",
        "- The exception is that the first layer name is changed.\n",
        "- x residual should be same shape of a prior layer shape.\n",
        "\n",
        "### 2-2) Bottleneck \n",
        "=> 52 layers, 101 layers, 152 layers\n",
        "- It is the same as in BasicBlock, but there is a 1x1 convolution, so more convolution layers should be considered."
      ],
      "metadata": {
        "id": "CPf-vrcDTmYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def conv1x1(ic, oc, s):\n",
        "    return nn.Conv2d(ic, oc, 1, s)\n",
        "\n",
        "def conv3x3(ic, oc, s, p):\n",
        "    return nn.Conv2d(ic, oc, 3, s, p) \n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, ln, bn, fn, s, p):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(fn*(2**(ln-1)), fn*(2**(ln)), s*2, p) if ln != 0 else conv3x3(fn*(2**(ln)), fn*(2**(ln)), s*2, p)  # just to prevent error     \n",
        "        self.conv2 = conv3x3(fn*(2**(ln)), fn*(2**(ln)), s, p)\n",
        "        self.skip  = nn.Sequential(\n",
        "            conv1x1(fn*(2**(ln-1)), fn*(2**(ln)), s*2) if ln != 0 else conv1x1(fn*(2**(ln)), fn*(2**(ln)), s*2),\n",
        "            nn.BatchNorm2d(fn*(2**(ln)))\n",
        "        )   \n",
        "        self.batch = nn.BatchNorm2d(fn*(2**ln))\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.ln = ln\n",
        "        self.bn = bn\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (self.ln != 0) and (self.bn == 1):\n",
        "            conv1 = self.conv1(x)\n",
        "        else:\n",
        "            conv1 = self.conv2(x)\n",
        "        batch1 = self.batch(conv1)\n",
        "        relu1  = self.relu(batch1)\n",
        "        conv2  = self.conv2(relu1)\n",
        "        batch2 = self.batch(conv2)\n",
        "        if (self.ln != 0) and (self.bn == 1):\n",
        "            add = torch.add(self.skip(x), batch2)\n",
        "        else:\n",
        "            add = torch.add(x, batch2)\n",
        "        relu2  = self.relu(add)\n",
        "        return relu2\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, ln, bn, fn, s, p):\n",
        "        super().__init__()\n",
        "        self.conv11= conv1x1(fn*(2**(ln)), fn*(2**(ln)), s)\n",
        "        self.conv12= conv1x1(fn*(2**(ln)*2), fn*(2**(ln)), s) \n",
        "        self.conv13= conv1x1(fn*(2**(ln)*4), fn*(2**(ln)), s)\n",
        "        self.conv2 = conv3x3(fn*(2**(ln)), fn*(2**(ln)), s*2, p) if (ln != 0) and (bn == 1) else conv3x3(fn*(2**(ln)), fn*(2**(ln)), s, p)\n",
        "        self.conv3 = conv1x1(fn*(2**(ln)), fn*(2**(ln))*4, s)\n",
        "        self.skip1 = nn.Sequential(\n",
        "            conv1x1(fn*(2**(ln)), fn*(2**(ln)*4), s),\n",
        "            nn.BatchNorm2d(fn*(2**ln)*4)                     \n",
        "        )\n",
        "        self.skip2 = nn.Sequential(\n",
        "            conv1x1(fn*(2**(ln)*2), fn*(2**(ln)*4), s*2),\n",
        "            nn.BatchNorm2d(fn*(2**ln)*4)                     \n",
        "        )\n",
        "        self.batch = nn.BatchNorm2d(fn*(2**ln))\n",
        "        self.batch4= nn.BatchNorm2d(fn*(2**ln)*4)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.ln = ln\n",
        "        self.bn = bn\n",
        "\n",
        "    def forward(self, x):\n",
        "        if (self.ln == 0) and (self.bn == 1):\n",
        "            conv1 = self.conv11(x)\n",
        "        elif (self.ln != 0) and (self.bn == 1):\n",
        "            conv1 = self.conv12(x)\n",
        "        else:\n",
        "            conv1 = self.conv13(x)\n",
        "        batch1 = self.batch(conv1)\n",
        "        relu1  = self.relu(batch1)\n",
        "        conv2  = self.conv2(relu1)\n",
        "        batch2 = self.batch(conv2)\n",
        "        relu2  = self.relu(batch2)\n",
        "        conv3  = self.conv3(relu2)\n",
        "        batch4 = self.batch4(conv3)\n",
        "        if (self.ln == 0) and (self.bn == 1):\n",
        "            add = torch.add(self.skip1(x), batch4)\n",
        "        elif (self.ln != 0) and (self.bn == 1):\n",
        "            add = torch.add(self.skip2(x), batch4)\n",
        "        else:\n",
        "            add = torch.add(x, batch4)\n",
        "        relu3  = self.relu(add)\n",
        "        return relu3\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    '''\n",
        "    ResNet is fully convolution neural network for image classification.\n",
        "\n",
        "    Args:\n",
        "        btype: Type of the block that will be used as an residual learning.\n",
        "            Available option: **BasicBlock** (for ResNet18, ResNet34) and **Bottleneck(ResNet50, ResNet101, ResNet152)**\n",
        "        block: List of the block number. (e.g ResNet34 -> [3,4,6,3])\n",
        "        fn: Number of features. default is 64.\n",
        "        s: stride.\n",
        "        p: padding.\n",
        "        classes: number of classes.\n",
        "    '''\n",
        "    def __init__(self, btype, block, fn, s, p, classes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,fn,7,2,3)\n",
        "        self.batch = nn.BatchNorm2d(fn)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxp1 = nn.MaxPool2d(2,2,0)\n",
        "        self.block1 = self._make_layer(btype, 0, block[0], fn, s, p)\n",
        "        self.block2 = self._make_layer(btype, 1, block[1], fn, s, p)\n",
        "        self.block3 = self._make_layer(btype, 2, block[2], fn, s, p)\n",
        "        self.block4 = self._make_layer(btype, 3, block[3], fn, s, p)\n",
        "        self.aapool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.flaten = nn.Flatten()\n",
        "        self.adjust = 1 if btype == 'BasicBlock' else 4\n",
        "        self.linear = nn.Linear(in_features=(fn*2**(len(block)-1)*self.adjust),out_features=classes)\n",
        "\n",
        "    def _make_layer(self, btype, ln, block, fn, s, p):\n",
        "        layers = []\n",
        "        for bn in range(1,block+1):\n",
        "            layers.append(BasicBlock(ln, bn, fn, s, p)) if btype == 'BasicBlock' else layers.append(Bottleneck(ln, bn, fn, s, p))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxp1(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.aapool(x)\n",
        "        x = self.flaten(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "def resnet18(btype='BasicBlock', block=[2,2,2,2], fn=64, s=1, p=1, classes=1000):\n",
        "    return ResNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def resnet34(btype='BasicBlock', block=[3,4,6,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return ResNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def resnet50(btype='Bottleneck', block=[3,4,6,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return ResNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def resnet101(btype='Bottleneck', block=[3,4,23,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return ResNet(btype, block, fn, s, p, classes)\n",
        "\n",
        "def resnet152(btype='Bottleneck', block=[3,8,36,3], fn=64, s=1, p=1, classes=1000):\n",
        "    return ResNet(btype, block, fn, s, p, classes)"
      ],
      "metadata": {
        "id": "Q9FTanvO8E5y"
      },
      "execution_count": 110,
      "outputs": []
    }
  ]
}